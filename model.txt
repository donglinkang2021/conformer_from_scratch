ModelCTC(
  (encoder): ConformerEncoder(
    (preprocessing): AudioPreprocessing(
      (Spectrogram): Spectrogram()
      (MelScale): MelScale()
    )
    (augment): SpecAugment()
    (subsampling_module): Conv2dSubsampling(
      (layers): ModuleList(
        (0): Sequential(
          (0): Conv2d(1, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Swish()
        )
        (1): Sequential(
          (0): Conv2d(176, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Swish()
        )
      )
    )
    (padding_mask): StreamingMask(
      (padding_mask): PaddingMask()
    )
    (linear): Linear(in_features=3520, out_features=176, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (1): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (2): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (3): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (4): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (5): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (6): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (7): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (8): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (9): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (10): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (11): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (12): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (13): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (14): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
      (15): ConformerBlock(
        (feed_forward_module1): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_self_attention_module): MultiHeadSelfAttentionModule(
          (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
          (mhsa): RelPosMultiHeadSelfAttention(
            (query_layer): Linear(in_features=176, out_features=176, bias=True)
            (key_layer): Linear(in_features=176, out_features=176, bias=True)
            (value_layer): Linear(in_features=176, out_features=176, bias=True)
            (output_layer): Linear(in_features=176, out_features=176, bias=True)
            (pos_layer): Linear(in_features=176, out_features=176, bias=True)
            (rel_pos_enc): RelativeSinusoidalPositionalEncoding()
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (convolution_module): ConvolutionModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Transpose()
            (2): Conv1d(176, 352, kernel_size=(1,), stride=(1,))
            (3): Glu()
            (4): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)
            (5): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): Swish()
            (7): Conv1d(176, 176, kernel_size=(1,), stride=(1,))
            (8): Transpose()
            (9): Dropout(p=0.1, inplace=False)
          )
        )
        (feed_forward_module2): FeedForwardModule(
          (layers): Sequential(
            (0): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
            (1): Linear(in_features=176, out_features=704, bias=True)
            (2): Swish()
            (3): Dropout(p=0.1, inplace=False)
            (4): Linear(in_features=704, out_features=176, bias=True)
            (5): Dropout(p=0.1, inplace=False)
          )
        )
        (norm): LayerNorm((176,), eps=1e-06, elementwise_affine=True)
        (att_res): Identity()
        (conv_res): Identity()
      )
    )
  )
  (fc): Linear(in_features=176, out_features=4350, bias=True)
  (criterion): LossCTC(
    (loss): CTCLoss()
  )
)